{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация сарказма\n",
    "\n",
    "В данном ноутбуке мы разберем классический случай NLP - задача классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    log_loss,\n",
    ")\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import src.utils as p\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала имортируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/Sarcasm_Headlines_Dataset_v2.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим лишние значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='article_link', inplace=True)\n",
    "df.rename(columns={'headline': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На случай важных переговоров\n",
    "# Если у вас не скачаны стоп слова\n",
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['text']=df['text'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) # Text that is Not Sarcastic\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 0].text))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) # Text that is Sarcastic\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 1].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Давайте обучим модель с помощью сервиса MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классический ML пайплайн\n",
    "\n",
    "Обучим обычный TF-iDF векторайзер, а также градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим переменные для MLFlow\n",
    "load_dotenv()\n",
    "\n",
    "EXPERIMENT_NAME = os.environ[\"EXPERIMENT_NAME\"]\n",
    "RUN_NAME = os.environ[\"RUN_NAME\"]\n",
    "REGISTRY_MODEL_NAME = os.environ[\"REGISTRY_MODEL_NAME\"]\n",
    "\n",
    "mlflow.set_tracking_uri(f\"http://{os.environ.get('MLFLOW_SERVER_HOST')}:{os.environ.get('MLFLOW_SERVER_PORT')}\")\n",
    "mlflow.set_registry_uri(f\"http://{os.environ.get('MLFLOW_SERVER_HOST')}:{os.environ.get('MLFLOW_SERVER_PORT')}\")\n",
    "\n",
    "if mlflow.get_experiment_by_name(name=EXPERIMENT_NAME):\n",
    "    experiment_id = dict(mlflow.get_experiment_by_name(name=EXPERIMENT_NAME))['experiment_id']\n",
    "    mlflow.set_experiment(experiment_id=experiment_id)\n",
    "else:\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    experiment_id = dict(mlflow.get_experiment_by_name(name=EXPERIMENT_NAME))['experiment_id']\n",
    "    mlflow.set_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'vect': {\n",
    "        'ngram_range': (1, 2),\n",
    "        'max_df': 0.95,\n",
    "        'min_df': 2\n",
    "    },\n",
    "    'model': {\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "import src.utils as utils\n",
    "wc1, wc2 = utils.create_cloud(df)\n",
    "mlflow.log_image(wc1.to_image(), \"non_sarcastic_cloud.png\")\n",
    "mlflow.log_image(wc2.to_image(), \"sarcastic_cloud.png\")\n",
    "\n",
    "vector = TfidfVectorizer(**params['vect'])\n",
    "X = vector.fit_transform(df['text'])\n",
    "# Split the data into training and test sets. (0.8, 0.20) split.\n",
    "X_train, X_test,  y_train, y_test = train_test_split(X, df['is_sarcastic'], test_size=0.2)\n",
    "\n",
    "pip_requirements=\"../requirements.txt\" \n",
    "signature = mlflow.models.infer_signature(X_test, y_test)\n",
    "input_example = X_test[:10]\n",
    "metadata = {\"model_type\": \"daily\"}\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME+\"_CatBoost\", experiment_id=experiment_id, nested=True): # ['experiment_id']\n",
    "    model = CatBoostClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    precision, recall, f_score, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_pred)\n",
    "    _, err1, err2, _ = confusion_matrix(y_test, y_pred, normalize='all').ravel()\n",
    "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    model_info = mlflow.catboost.log_model(\n",
    "        cb_model=model,\n",
    "        artifact_path=\"via_models\",\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        registered_model_name=REGISTRY_MODEL_NAME,\n",
    "        await_registration_for=60,\n",
    "        pip_requirements=pip_requirements,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "    mlflow.log_params(params=params)\n",
    "    mlflow.log_metric(\"precision\", precision[1])\n",
    "    mlflow.log_metric(\"recall\", recall[1])\n",
    "    mlflow.log_metric(\"f_score\", f_score[1])\n",
    "    mlflow.log_metric(\"logloss\", logloss)\n",
    "    mlflow.log_metric(\"err1\", err1)\n",
    "    mlflow.log_metric(\"err2\", err2)\n",
    "    mlflow.log_metric(\"auc\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "import src.utils as utils\n",
    "wc1, wc2 = utils.create_cloud(df)\n",
    "mlflow.log_image(wc1.to_image(), \"non_sarcastic_cloud.png\")\n",
    "mlflow.log_image(wc2.to_image(), \"sarcastic_cloud.png\")\n",
    "\n",
    "# Split the data into training and test sets. (0.8, 0.20) split.\n",
    "X_train, X_test,  y_train, y_test = train_test_split(df.drop(columns='is_sarcastic'), df['is_sarcastic'], test_size=0.2)\n",
    "\n",
    "pip_requirements=\"./requirements.txt\" \n",
    "signature = mlflow.models.infer_signature(X_test, y_test)\n",
    "input_example = X_test[:10]\n",
    "metadata = {\"model_type\": \"daily\"}\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME+\"_Sklearn\", experiment_id=experiment_id, nested=True): # ['experiment_id']\n",
    "    ct = ColumnTransformer([('vect', TfidfVectorizer(**params['vect']), 'text')])\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            ('transformer', ct),\n",
    "            ('model', CatBoostClassifier(**params['model']))\n",
    "        ]\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    precision, recall, f_score, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_pred)\n",
    "    _, err1, err2, _ = confusion_matrix(y_test, y_pred, normalize='all').ravel()\n",
    "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"via_models\",\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        registered_model_name=REGISTRY_MODEL_NAME,\n",
    "        await_registration_for=60,\n",
    "        pip_requirements=pip_requirements,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "    mlflow.log_params(params=params)\n",
    "    mlflow.log_metric(\"precision\", precision[1])\n",
    "    mlflow.log_metric(\"recall\", recall[1])\n",
    "    mlflow.log_metric(\"f_score\", f_score[1])\n",
    "    mlflow.log_metric(\"logloss\", logloss)\n",
    "    mlflow.log_metric(\"err1\", err1)\n",
    "    mlflow.log_metric(\"err2\", err2)\n",
    "    mlflow.log_metric(\"auc\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим переменные для MLFlow\n",
    "\n",
    "EXPERIMENT_NAME = \"churn_S2W2\"\n",
    "RUN_NAME = \"model_sarcasm_classic\"\n",
    "REGISTRY_MODEL_NAME = \"churn_model_sarcasm\"\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://storage.yandexcloud.net\"\n",
    "os.environ[\"AWS_BUCKET_NAME\"] = str(os.environ.get('S3_BUCKET_NAME'))\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = str(os.environ.get('AWS_ACCESS_KEY_ID'))\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = str(os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "mlflow.set_tracking_uri(f\"http://{os.environ.get('MLFLOW_SERVER_HOST')}:{os.environ.get('MLFLOW_SERVER_PORT')}\")\n",
    "mlflow.set_registry_uri(f\"http://{os.environ.get('MLFLOW_SERVER_HOST')}:{os.environ.get('MLFLOW_SERVER_PORT')}\")\n",
    "\n",
    "if mlflow.get_experiment_by_name(name=EXPERIMENT_NAME):\n",
    "    experiment_id = dict(mlflow.get_experiment_by_name(name=EXPERIMENT_NAME))['experiment_id']\n",
    "else:\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    experiment_id = dict(mlflow.get_experiment_by_name(name=EXPERIMENT_NAME))['experiment_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vect_trial(trial: Trial, vect_type: str) -> Dict:\n",
    "    # logging.info('Get vectorizetion parameters - Start')\n",
    "    if vect_type == 'count_vect':\n",
    "        vect_params = {\n",
    "            'type': \"count_vect\",\n",
    "            \"ngram_range\": trial.suggest_categorical('count_vect.ngram_range', \n",
    "                                                     [(1, 1), (1, 2)]),\n",
    "            \"min_df\": trial.suggest_int('count_vect.min_df', 1, 5),\n",
    "            \"max_df\": trial.suggest_float('count_vect.max_df', 0.7, 0.95)                                      \n",
    "        }\n",
    "    elif vect_type == 'tfidf':\n",
    "        vect_params = {\n",
    "            'type': \"tfidf\",\n",
    "            \"ngram_range\": trial.suggest_categorical('tfidf_vect.ngram_range', \n",
    "                                                     [(1, 1), (1, 2)]),\n",
    "            \"min_df\": trial.suggest_int('tfidf_vect.min_df', 1, 5),\n",
    "            \"max_df\": trial.suggest_float('tfidf_vect.max_df', 0.7, 0.95),\n",
    "            \"sublinear_tf\": trial.suggest_categorical('tfidf_vect.sublinear_tf',\n",
    "                                                      [True, False])                                    \n",
    "        }\n",
    "    # logging.info('Get vectorizetion parameters - End')\n",
    "    return vect_params\n",
    "\n",
    "def get_trial_params(trial: Trial) -> Dict:\n",
    "    # logging.info('Get full trial parameters - Start')\n",
    "    vect_space = trial.suggest_categorical('vectorizer_type',\n",
    "                                           ['count_vect', 'tfidf'])\n",
    "    vect_params = get_vect_trial(trial=trial, vect_type=vect_space)\n",
    "\n",
    "    model_space = trial.suggest_categorical('classifier_type',\n",
    "                                            ['catboost', 'logreg'])\n",
    "    if model_space == 'catboost':\n",
    "        model_params = {\n",
    "            'type': 'catboost',\n",
    "            'max_depth': trial.suggest_int('catboost.max_depth', 2, 5),\n",
    "            'n_estimators': trial.suggest_int('catboost.n_estimators', 50, 300),\n",
    "            'learning_rate': trial.suggest_float('catboost.learning_rate', 0.001, 1),\n",
    "            'reg_lambda': trial.suggest_float('catboost.learning_rate', 0.001, 1),\n",
    "            'auto_class_weights': trial.suggest_categorical('catboost.auto_class_weights',\n",
    "                                                            ['Balanced', None]),\n",
    "        }\n",
    "    elif model_space == 'logreg':\n",
    "        model_params = {\n",
    "            'type': 'logreg',\n",
    "            'class_weight': trial.suggest_categorical('logreg.class_weight',\n",
    "                                                      ['balanced', None]),\n",
    "            'penalty': trial.suggest_categorical('logreg.penalty',\n",
    "                                                 ['l1', 'l2', None]),\n",
    "            'C': trial.suggest_float('logreg.C', 0.1, 100)\n",
    "        }\n",
    "    \n",
    "    full_space = {\n",
    "        'vect': vect_params,\n",
    "        'model': model_params\n",
    "    }\n",
    "    # logging.info('Get full trial parameters - End')\n",
    "    return full_space\n",
    "\n",
    "def get_optuna_pipeline(space: Dict):\n",
    "    # logging.info('Get optuna pipeline - Start')\n",
    "    vect_type = space['vect']['type']\n",
    "    model_type = space['model']['type']\n",
    "    del space['model']['type'], space['vect']['type']\n",
    "\n",
    "    if vect_type == 'count_vect':\n",
    "        vect = CountVectorizer(**space['vect'])\n",
    "    elif vect_type == 'tfidf':\n",
    "        vect = TfidfVectorizer(**space['vect'])\n",
    "    else:\n",
    "        logging.info(f\"Don't match vect_type == {vect_type}\")\n",
    "\n",
    "    if model_type == 'catboost':\n",
    "        model = CatBoostClassifier(**space['model'], verbose=False)\n",
    "    elif model_type == 'logreg':\n",
    "        solver = 'lbfgs'\n",
    "        if space['model']['penalty'] == 'l1':\n",
    "            solver = 'saga'\n",
    "        model = LogisticRegression(**space['model'], solver=solver, random_state=42)\n",
    "    else:\n",
    "        logging.info(f\"Don't match model_type == {model_type}\")\n",
    "\n",
    "    # logging.info('Get optuna pipeline - End')\n",
    "    return build_pipeline(vect, model)\n",
    "\n",
    "def build_pipeline(vect, model):\n",
    "    # logging.info('Build Pipeline - Start')\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            ('vect', vect),\n",
    "            ('model', model)\n",
    "        ]\n",
    "    )\n",
    "    # logging.info('Build Pipeline - End')\n",
    "    return pipe\n",
    "\n",
    "def collect_optuna_metrics(trial: Trial):\n",
    "    # logging.info(f'Start collect metrics trial {trial.number}')\n",
    "    with mlflow.start_run(run_name=RUN_NAME+\"_optuna_\"+str(trial.number), nested=True):\n",
    "        df = get_optuna_data()\n",
    "        X = df['text']\n",
    "        y = df['is_sarcastic']\n",
    "        full_space = get_trial_params(trial=trial)\n",
    "        pipeline = get_optuna_pipeline(full_space.copy())\n",
    "        cv = StratifiedKFold(n_splits=5)\n",
    "        cv_f1_macro = []\n",
    "        cv_f1_micro = []\n",
    "        cv_accuracy = []\n",
    "        cv_logloss = []\n",
    "        cv_f1_1_score = []\n",
    "        cv_precision_1_score = []\n",
    "        cv_recall_1_score = []\n",
    "        overfit_penalty = []\n",
    "        for i, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "            X_train = X[train_idx]\n",
    "            y_train = y[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            y_test = y[test_idx]\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            predict = pipeline.predict(X_test)\n",
    "            overfit_penalty.append(\n",
    "                f1_score(y_true=y_train, y_pred=pipeline.predict(X_train), labels=[1]) -  f1_score(y_true=y_test, y_pred=predict, labels=[1])\n",
    "            )\n",
    "            cv_f1_macro.append(f1_score(y_true=y_test, y_pred=predict, average='macro'))\n",
    "            cv_f1_micro.append(f1_score(y_true=y_test, y_pred=predict, average='micro'))\n",
    "            cv_accuracy.append(accuracy_score(y_true=y_test, y_pred=predict))\n",
    "            cv_logloss.append(log_loss(y_true=y_test, y_pred=predict))\n",
    "            cv_f1_1_score.append(f1_score(y_true=y_test, y_pred=predict, labels=[1]))\n",
    "            cv_precision_1_score.append(precision_score(y_true=y_test, y_pred=predict, labels=[1]))\n",
    "            cv_recall_1_score.append(recall_score(y_true=y_test, y_pred=predict, labels=[1]))\n",
    "        \n",
    "        answer_info = {\n",
    "            'overfit_penalty': np.mean(overfit_penalty),\n",
    "            'cv_f1_macro': np.mean(cv_f1_macro),\n",
    "            'cv_f1_micro': np.mean(cv_f1_micro),\n",
    "            'cv_accuracy': np.mean(cv_accuracy),\n",
    "            'cv_logloss': np.mean(cv_logloss),\n",
    "            'cv_f1_1_score': np.mean(cv_f1_1_score),\n",
    "            'cv_precision_1_score': np.mean(cv_precision_1_score),\n",
    "            'cv_recall_1_score': np.mean(cv_recall_1_score),\n",
    "            'status': optuna.trial.TrialState.COMPLETE\n",
    "        }\n",
    "        if np.mean(cv_accuracy) or np.mean(cv_accuracy) == 1.0 or np.mean(overfit_penalty) > 0.3:\n",
    "            answer_info.update({'status': optuna.trial.TrialState.FAIL})\n",
    "        mlflow.log_params(full_space)\n",
    "        mlflow.log_metrics(answer_info)\n",
    "        # trial.set_user_attr('report', answer_info)\n",
    "        # logging.info(f'End collect metrics trial {trial.number}')\n",
    "    return np.mean(cv_f1_1_score)\n",
    "\n",
    "def optuna_objective(trial: Trial, X, y):\n",
    "    full_space = get_trial_params(trial=trial)\n",
    "    pipeline = get_optuna_pipeline(full_space)\n",
    "    return collect_optuna_metrics(pipeline=pipeline, trial=trial, X=X, y=y, full_space=full_space)\n",
    "\n",
    "def get_optuna_data():\n",
    "    df = pd.read_json('./data/Sarcasm_Headlines_Dataset_v2.json', lines=True)\n",
    "    df.drop(columns='article_link', inplace=True)\n",
    "    df.rename(columns={'headline': 'text'}, inplace=True)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop.update(punctuation)\n",
    "    def strip_html(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "    #Removing the square brackets\n",
    "    def remove_between_square_brackets(text):\n",
    "        return re.sub('\\[[^]]*\\]', '', text)\n",
    "    # Removing URL's\n",
    "    def remove_between_square_brackets(text):\n",
    "        return re.sub(r'http\\S+', '', text)\n",
    "    #Removing the stopwords from text\n",
    "    def remove_stopwords(text):\n",
    "        final_text = []\n",
    "        for i in text.split():\n",
    "            if i.strip().lower() not in stop:\n",
    "                final_text.append(i.strip())\n",
    "        return \" \".join(final_text)\n",
    "    #Removing the noisy text\n",
    "    def denoise_text(text):\n",
    "        text = strip_html(text)\n",
    "        text = remove_between_square_brackets(text)\n",
    "        text = remove_stopwords(text)\n",
    "        return text\n",
    "    #Apply function on review column\n",
    "    df['text']=df['text'].apply(denoise_text)\n",
    "    return df\n",
    "\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_optuna_data()\n",
    "X = df['text']\n",
    "y = df['is_sarcastic']\n",
    "\n",
    "pip_requirements=\"./requirements.txt\" \n",
    "signature = mlflow.models.infer_signature(df)\n",
    "input_example = df[:10]\n",
    "metadata = {\"model_type\": \"daily\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id, run_name=RUN_NAME+\"_optuna\"):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(collect_optuna_metrics, n_trials=10, callbacks=[champion_callback])\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_precision\", study.best_value)\n",
    "\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            'project': EXPERIMENT_NAME,\n",
    "            'engine': 'optuna',\n",
    "            'version': 1\n",
    "        }\n",
    "    )\n",
    "    def get_params_after_learning(params, key: str) -> Dict:\n",
    "        _params = {}\n",
    "        for _key in study.best_params.keys():\n",
    "            if _key.startswith(key):\n",
    "                _key = _key.replace(key, '')\n",
    "                _params[_key] = params[key+_key]\n",
    "        return _params\n",
    "\n",
    "    if study.best_params['vectorizer_type'] == 'tfidf':\n",
    "        vect_params = get_params_after_learning(study.best_params, 'tfidf_vect.')\n",
    "        vect = TfidfVectorizer(**vect_params)\n",
    "    else:\n",
    "        vect_params = get_params_after_learning(study.best_params, 'count_vect.')\n",
    "        vect = CountVectorizer(**vect_params)\n",
    "\n",
    "    if study.best_params['classifier_type'] == 'logreg':\n",
    "        model_params = get_params_after_learning(study.best_params, 'logreg.')\n",
    "        model_params['solver'] = 'lbfgs'\n",
    "        if model_params['penalty'] == 'l1':\n",
    "            model_params['solver'] = ('saga')\n",
    "        model = LogisticRegression(**model_params, random_state=42)\n",
    "    else:\n",
    "        model_params = get_params_after_learning(study.best_params, 'catboost.')\n",
    "        model = CatBoostClassifier(**model_params)\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            ('vect', vect), \n",
    "            ('model', model)\n",
    "        ]\n",
    "    )\n",
    "    pipe.fit(X=X, y=y)\n",
    "\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"via_models\",\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        registered_model_name=REGISTRY_MODEL_NAME+'_tune',\n",
    "        await_registration_for=60,\n",
    "        pip_requirements=pip_requirements,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "    mlflow.log_param('vect_type', study.best_params['vectorizer_type'])\n",
    "    mlflow.log_params(vect_params)\n",
    "    mlflow.log_param('model_type', study.best_params['classifier_type'])\n",
    "    mlflow.log_params(model_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
